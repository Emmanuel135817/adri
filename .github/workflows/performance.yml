name: Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark memory-profiler psutil bc
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install -e .

    - name: Create benchmark test data
      run: |
        mkdir -p benchmark_data
        python -c "
        import pandas as pd
        import json

        # Create test datasets of various sizes
        sizes = [100, 1000, 10000, 50000]
        for size in sizes:
            # CSV data
            df = pd.DataFrame({
                'id': range(size),
                'name': [f'user_{i}' for i in range(size)],
                'email': [f'user_{i}@example.com' for i in range(size)],
                'age': [20 + (i % 60) for i in range(size)],
                'score': [0.1 * (i % 100) for i in range(size)]
            })
            df.to_csv(f'benchmark_data/test_data_{size}.csv', index=False)

            # JSON data
            json_data = df.to_dict('records')
            with open(f'benchmark_data/test_data_{size}.json', 'w') as f:
                json.dump(json_data, f)

        print('Benchmark test data created')
        "

    - name: Run Core Performance Benchmarks
      run: |
        echo "‚ö° Running core performance benchmarks..."
        python -m pytest tests/test_benchmarks.py -v --benchmark-only --benchmark-json=benchmark-results.json --no-cov || true

    - name: Run Decorator Performance Tests
      run: |
        echo "üéØ Testing @adri_protected decorator performance..."
        python -c "
        import time
        import pandas as pd
        from adri.decorators.guard import adri_protected
        import statistics

        # Test decorator overhead
        @adri_protected(data_param='data')
        def protected_function(data):
            return len(data)

        def unprotected_function(data):
            return len(data)

        # Test with different data sizes
        sizes = [100, 1000, 10000]
        results = []

        for size in sizes:
            test_data = pd.DataFrame({'col': range(size)})

            # Measure unprotected function
            times_unprotected = []
            for _ in range(10):
                start = time.perf_counter()
                unprotected_function(test_data)
                end = time.perf_counter()
                times_unprotected.append(end - start)

            # Measure protected function
            times_protected = []
            for _ in range(10):
                start = time.perf_counter()
                protected_function(test_data)
                end = time.perf_counter()
                times_protected.append(end - start)

            avg_unprotected = statistics.mean(times_unprotected)
            avg_protected = statistics.mean(times_protected)
            overhead = ((avg_protected - avg_unprotected) / avg_unprotected) * 100

            results.append({
                'size': size,
                'unprotected_ms': avg_unprotected * 1000,
                'protected_ms': avg_protected * 1000,
                'overhead_percent': overhead
            })

            print(f'Size {size}: Unprotected={avg_unprotected*1000:.2f}ms, Protected={avg_protected*1000:.2f}ms, Overhead={overhead:.1f}%')

        # Save results
        import json
        with open('decorator-benchmark.json', 'w') as f:
            json.dump(results, f, indent=2)
        "

    - name: Run CLI Performance Tests
      run: |
        echo "üíª Testing CLI command performance..."
        python -c "
        import subprocess
        import time
        import json

        # Test CLI commands with different data sizes
        sizes = [100, 1000, 10000]
        cli_results = []

        for size in sizes:
            # Test assess command
            start = time.perf_counter()
            result = subprocess.run([
                'python', '-m', 'adri.cli.commands', 'assess',
                f'benchmark_data/test_data_{size}.csv',
                '--standard', 'adri/templates/adri_assessment_report_standard_v0.1.0.yaml'
            ], capture_output=True, text=True)
            end = time.perf_counter()

            execution_time = end - start
            cli_results.append({
                'command': 'assess',
                'data_size': size,
                'execution_time_ms': execution_time * 1000,
                'success': result.returncode == 0
            })

            print(f'CLI assess (size {size}): {execution_time*1000:.2f}ms, Success: {result.returncode == 0}')

        # Save CLI results
        with open('cli-benchmark.json', 'w') as f:
            json.dump(cli_results, f, indent=2)
        "

    - name: Run Memory Usage Tests
      run: |
        echo "üß† Testing memory usage patterns..."
        python -c "
        import psutil
        import pandas as pd
        from adri.core.assessor import DataQualityAssessor
        import json

        def get_memory_usage():
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB

        # Test memory usage with different data sizes
        sizes = [1000, 10000, 50000]
        memory_results = []

        for size in sizes:
            # Create test data
            test_data = pd.DataFrame({
                'col1': range(size),
                'col2': [f'text_{i}' for i in range(size)],
                'col3': [i * 0.1 for i in range(size)]
            })

            # Measure memory before
            memory_before = get_memory_usage()

            # Run assessment
            assessor = DataQualityAssessor()
            try:
                result = assessor.assess_data_quality(test_data, {})
                memory_after = get_memory_usage()

                memory_used = memory_after - memory_before
                memory_results.append({
                    'data_size': size,
                    'memory_before_mb': memory_before,
                    'memory_after_mb': memory_after,
                    'memory_used_mb': memory_used,
                    'memory_per_row_kb': (memory_used * 1024) / size
                })

                print(f'Size {size}: Memory used={memory_used:.2f}MB, Per row={memory_used*1024/size:.2f}KB')
            except Exception as e:
                print(f'Error with size {size}: {e}')

        # Save memory results
        with open('memory-benchmark.json', 'w') as f:
            json.dump(memory_results, f, indent=2)
        "

    - name: Upload Benchmark Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-reports
        path: |
          benchmark-results.json
          decorator-benchmark.json
          cli-benchmark.json
          memory-benchmark.json

    - name: Performance Summary
      if: always()
      run: |
        echo "## ‚ö° Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Decorator Performance
        if [ -f decorator-benchmark.json ]; then
          echo "### üéØ @adri_protected Decorator Performance" >> $GITHUB_STEP_SUMMARY
          python -c "
        import json
        with open('decorator-benchmark.json', 'r') as f:
            data = json.load(f)

        print('| Data Size | Unprotected (ms) | Protected (ms) | Overhead (%) |')
        print('|-----------|------------------|----------------|--------------|')
        for result in data:
            print(f'| {result[\"size\"]:,} | {result[\"unprotected_ms\"]:.2f} | {result[\"protected_ms\"]:.2f} | {result[\"overhead_percent\"]:.1f}% |')
          " >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # CLI Performance
        if [ -f cli-benchmark.json ]; then
          echo "### üíª CLI Command Performance" >> $GITHUB_STEP_SUMMARY
          python -c "
        import json
        with open('cli-benchmark.json', 'r') as f:
            data = json.load(f)

        print('| Command | Data Size | Execution Time (ms) | Success |')
        print('|---------|-----------|-------------------|---------|')
        for result in data:
            success_icon = '‚úÖ' if result['success'] else '‚ùå'
            print(f'| {result[\"command\"]} | {result[\"data_size\"]:,} | {result[\"execution_time_ms\"]:.2f} | {success_icon} |')
          " >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Memory Usage
        if [ -f memory-benchmark.json ]; then
          echo "### üß† Memory Usage Analysis" >> $GITHUB_STEP_SUMMARY
          python -c "
        import json
        with open('memory-benchmark.json', 'r') as f:
            data = json.load(f)

        print('| Data Size | Memory Used (MB) | Per Row (KB) |')
        print('|-----------|------------------|--------------|')
        for result in data:
            print(f'| {result[\"data_size\"]:,} | {result[\"memory_used_mb\"]:.2f} | {result[\"memory_per_row_kb\"]:.3f} |')
          " >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        echo "### üìä Performance Standards" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ Decorator overhead should be < 10%" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ CLI commands should complete < 5 seconds for 10K rows" >> $GITHUB_STEP_SUMMARY
        echo "- ‚úÖ Memory usage should be < 1KB per row" >> $GITHUB_STEP_SUMMARY

    - name: Check Performance Regression
      run: |
        echo "üîç Checking for performance regressions..."

        # Check decorator absolute performance (not percentage overhead)
        if [ -f decorator-benchmark.json ]; then
          MAX_PROTECTED_TIME=$(python -c "
        import json
        with open('decorator-benchmark.json', 'r') as f:
            data = json.load(f)
        max_time = max(result['protected_ms'] for result in data)
        print(max_time)
          ")

          if (( $(echo "$MAX_PROTECTED_TIME > 100.0" | bc -l) )); then
            echo "‚ùå Performance regression detected: Decorator takes ${MAX_PROTECTED_TIME}ms (max allowed: 100ms)"
            exit 1
          else
            echo "‚úÖ Decorator performance acceptable: ${MAX_PROTECTED_TIME}ms"
          fi
        fi

        # Check CLI performance
        if [ -f cli-benchmark.json ]; then
          MAX_CLI_TIME=$(python -c "
        import json
        with open('cli-benchmark.json', 'r') as f:
            data = json.load(f)
        max_time = max(result['execution_time_ms'] for result in data if result['data_size'] == 10000)
        print(max_time)
          ")

          if (( $(echo "$MAX_CLI_TIME > 5000" | bc -l) )); then
            echo "‚ùå Performance regression detected: CLI command took ${MAX_CLI_TIME}ms (max allowed: 5000ms)"
            exit 1
          else
            echo "‚úÖ CLI performance acceptable: ${MAX_CLI_TIME}ms"
          fi
        fi

        echo "‚úÖ No performance regressions detected"

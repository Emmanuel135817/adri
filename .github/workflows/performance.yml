name: Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark pytest-timeout memory-profiler psutil bc tabulate pyyaml requests
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install -e .

    - name: Download Previous Benchmark Results
      if: github.event_name == 'pull_request' || github.event_name == 'push'
      continue-on-error: true
      run: |
        echo "📥 Attempting to download previous benchmark results..."
        python scripts/download_previous_benchmark.py \
          --repo "${{ github.repository }}" \
          --branch "${{ github.base_ref || 'main' }}" \
          --workflow "performance.yml" \
          --artifact "benchmark-results" \
          --output "previous-benchmark.json" \
          --token "${{ secrets.GITHUB_TOKEN }}" || echo "No previous benchmark found"

    - name: Create benchmark test data
      run: |
        mkdir -p benchmark_data
        python -c "
        import pandas as pd
        import json

        # Create test datasets of various sizes
        sizes = [100, 1000, 10000, 50000]
        for size in sizes:
            # CSV data
            df = pd.DataFrame({
                'id': range(size),
                'name': [f'user_{i}' for i in range(size)],
                'email': [f'user_{i}@example.com' for i in range(size)],
                'age': [20 + (i % 60) for i in range(size)],
                'score': [0.1 * (i % 100) for i in range(size)]
            })
            df.to_csv(f'benchmark_data/test_data_{size}.csv', index=False)

            # JSON data
            json_data = df.to_dict('records')
            with open(f'benchmark_data/test_data_{size}.json', 'w') as f:
                json.dump(json_data, f)

        print('Benchmark test data created')
        "

    - name: Run Core Performance Benchmarks with Timeout
      run: |
        echo "⚡ Running core performance benchmarks with timeout protection..."
        python -m pytest tests/test_benchmarks.py -v \
          --performance \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --timeout=60 \
          --timeout-method=thread \
          --no-cov || true

    - name: Add Metadata to Benchmark Results
      run: |
        python -c "
        import json
        from datetime import datetime

        # Load benchmark results
        with open('benchmark-results.json', 'r') as f:
            data = json.load(f)

        # Add metadata
        data['commit_sha'] = '${{ github.sha }}'
        data['branch'] = '${{ github.ref_name }}'
        data['timestamp'] = datetime.utcnow().isoformat()
        data['workflow'] = 'performance.yml'

        # Save updated results
        with open('benchmark-results.json', 'w') as f:
            json.dump(data, f, indent=2)
        "

    - name: Run Decorator Performance Tests
      run: |
        echo "🎯 Testing @adri_protected decorator performance..."
        python -c "
        import time
        import pandas as pd
        from adri.decorators.guard import adri_protected
        import statistics
        import json

        # Test decorator overhead
        @adri_protected(data_param='data')
        def protected_function(data):
            return len(data)

        def unprotected_function(data):
            return len(data)

        # Test with different data sizes
        sizes = [100, 1000, 10000]
        results = []

        for size in sizes:
            test_data = pd.DataFrame({'col': range(size)})

            # Measure unprotected function
            times_unprotected = []
            for _ in range(10):
                start = time.perf_counter()
                unprotected_function(test_data)
                end = time.perf_counter()
                times_unprotected.append(end - start)

            # Measure protected function
            times_protected = []
            for _ in range(10):
                start = time.perf_counter()
                protected_function(test_data)
                end = time.perf_counter()
                times_protected.append(end - start)

            avg_unprotected = statistics.mean(times_unprotected)
            avg_protected = statistics.mean(times_protected)
            overhead = ((avg_protected - avg_unprotected) / avg_unprotected) * 100

            results.append({
                'size': size,
                'unprotected_ms': avg_unprotected * 1000,
                'protected_ms': avg_protected * 1000,
                'overhead_percent': overhead
            })

            print(f'Size {size}: Unprotected={avg_unprotected*1000:.2f}ms, Protected={avg_protected*1000:.2f}ms, Overhead={overhead:.1f}%')

        # Save results
        with open('decorator-benchmark.json', 'w') as f:
            json.dump(results, f, indent=2)
        "

    - name: Run CLI Performance Tests
      run: |
        echo "💻 Testing CLI command performance..."
        python -c "
        import subprocess
        import time
        import json

        # Test CLI commands with different data sizes
        sizes = [100, 1000, 10000]
        cli_results = []

        for size in sizes:
            # Test assess command
            start = time.perf_counter()
            result = subprocess.run([
                'python', '-m', 'adri.cli.commands', 'assess',
                f'benchmark_data/test_data_{size}.csv',
                '--standard', 'adri/templates/adri_assessment_report_standard_v0.1.0.yaml'
            ], capture_output=True, text=True)
            end = time.perf_counter()

            execution_time = end - start
            cli_results.append({
                'command': 'assess',
                'data_size': size,
                'execution_time_ms': execution_time * 1000,
                'success': result.returncode == 0
            })

            print(f'CLI assess (size {size}): {execution_time*1000:.2f}ms, Success: {result.returncode == 0}')

        # Save CLI results
        with open('cli-benchmark.json', 'w') as f:
            json.dump(cli_results, f, indent=2)
        "

    - name: Run Memory Usage Tests
      run: |
        echo "🧠 Testing memory usage patterns..."
        python -c "
        import psutil
        import pandas as pd
        from adri.core.assessor import DataQualityAssessor
        import json

        def get_memory_usage():
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB

        # Test memory usage with different data sizes
        sizes = [1000, 10000, 50000]
        memory_results = []

        for size in sizes:
            # Create test data
            test_data = pd.DataFrame({
                'col1': range(size),
                'col2': [f'text_{i}' for i in range(size)],
                'col3': [i * 0.1 for i in range(size)]
            })

            # Measure memory before
            memory_before = get_memory_usage()

            # Run assessment
            assessor = DataQualityAssessor()
            try:
                result = assessor.assess(test_data)
                memory_after = get_memory_usage()

                memory_used = memory_after - memory_before
                memory_results.append({
                    'data_size': size,
                    'memory_before_mb': memory_before,
                    'memory_after_mb': memory_after,
                    'memory_used_mb': memory_used,
                    'memory_per_row_kb': (memory_used * 1024) / size
                })

                print(f'Size {size}: Memory used={memory_used:.2f}MB, Per row={memory_used*1024/size:.2f}KB')
            except Exception as e:
                print(f'Error with size {size}: {e}')

        # Save memory results
        with open('memory-benchmark.json', 'w') as f:
            json.dump(memory_results, f, indent=2)
        "

    - name: Compare Benchmark Results
      if: always()
      run: |
        echo "📊 Comparing benchmark results..."

        # Check if previous benchmark exists
        if [ -f "previous-benchmark.json" ]; then
          python scripts/compare_benchmarks.py \
            benchmark-results.json \
            --previous previous-benchmark.json \
            --thresholds .github/benchmark-thresholds.yml \
            --output comparison-report.md \
            --github-output || echo "Comparison completed with warnings"
        else
          echo "No previous benchmark found, skipping comparison"
          python scripts/compare_benchmarks.py \
            benchmark-results.json \
            --thresholds .github/benchmark-thresholds.yml \
            --output comparison-report.md \
            --github-output || echo "Threshold check completed"
        fi

    - name: Upload Benchmark Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          benchmark-results.json
          decorator-benchmark.json
          cli-benchmark.json
          memory-benchmark.json
          comparison-report.md
        retention-days: 30

    - name: Upload Previous Benchmark for Cache
      uses: actions/upload-artifact@v4
      if: always() && github.ref == 'refs/heads/main'
      continue-on-error: true
      with:
        name: benchmark-cache-${{ github.sha }}
        path: benchmark-results.json
        retention-days: 90

    - name: Performance Summary
      if: always()
      run: |
        echo "## ⚡ Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Add comparison report if it exists
        if [ -f comparison-report.md ]; then
          cat comparison-report.md >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Decorator Performance
        if [ -f decorator-benchmark.json ]; then
          echo "### 🎯 @adri_protected Decorator Performance" >> $GITHUB_STEP_SUMMARY
          python -c "
        import json
        with open('decorator-benchmark.json', 'r') as f:
            data = json.load(f)

        print('| Data Size | Unprotected (ms) | Protected (ms) | Overhead (%) |')
        print('|-----------|------------------|----------------|--------------|')
        for result in data:
            print(f'| {result[\"size\"]:,} | {result[\"unprotected_ms\"]:.2f} | {result[\"protected_ms\"]:.2f} | {result[\"overhead_percent\"]:.1f}% |')
          " >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # CLI Performance
        if [ -f cli-benchmark.json ]; then
          echo "### 💻 CLI Command Performance" >> $GITHUB_STEP_SUMMARY
          python -c "
        import json
        with open('cli-benchmark.json', 'r') as f:
            data = json.load(f)

        print('| Command | Data Size | Execution Time (ms) | Success |')
        print('|---------|-----------|-------------------|---------|')
        for result in data:
            success_icon = '✅' if result['success'] else '❌'
            print(f'| {result[\"command\"]} | {result[\"data_size\"]:,} | {result[\"execution_time_ms\"]:.2f} | {success_icon} |')
          " >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Memory Usage
        if [ -f memory-benchmark.json ]; then
          echo "### 🧠 Memory Usage Analysis" >> $GITHUB_STEP_SUMMARY
          python -c "
        import json
        with open('memory-benchmark.json', 'r') as f:
            data = json.load(f)

        print('| Data Size | Memory Used (MB) | Per Row (KB) |')
        print('|-----------|------------------|--------------|')
        for result in data:
            print(f'| {result[\"data_size\"]:,} | {result[\"memory_used_mb\"]:.2f} | {result[\"memory_per_row_kb\"]:.3f} |')
          " >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Check Performance Against Thresholds
      run: |
        echo "🔍 Checking performance against thresholds..."

        # Load threshold configuration
        python -c "
        import json
        import yaml
        import sys
        import os

        # Load thresholds
        with open('.github/benchmark-thresholds.yml', 'r') as f:
            config = yaml.safe_load(f)
        thresholds = config['thresholds']

        violations = []

        # Check decorator overhead
        if os.path.exists('decorator-benchmark.json'):
            with open('decorator-benchmark.json', 'r') as f:
                decorator_data = json.load(f)

            for result in decorator_data:
                if result['overhead_percent'] > thresholds['decorator_overhead_percent']:
                    violations.append(f'Decorator overhead {result[\"overhead_percent\"]:.1f}% exceeds threshold {thresholds[\"decorator_overhead_percent\"]}%')
                if result['protected_ms'] > thresholds['max_protected_time_ms']:
                    violations.append(f'Protected function time {result[\"protected_ms\"]:.2f}ms exceeds threshold {thresholds[\"max_protected_time_ms\"]}ms')

        # Check CLI performance
        if os.path.exists('cli-benchmark.json'):
            with open('cli-benchmark.json', 'r') as f:
                cli_data = json.load(f)

            for result in cli_data:
                if result['data_size'] == 10000 and result['execution_time_ms'] > thresholds['cli_max_time_10k_rows_ms']:
                    violations.append(f'CLI execution time {result[\"execution_time_ms\"]:.2f}ms exceeds threshold {thresholds[\"cli_max_time_10k_rows_ms\"]}ms')

        # Check memory usage
        if os.path.exists('memory-benchmark.json'):
            with open('memory-benchmark.json', 'r') as f:
                memory_data = json.load(f)

            for result in memory_data:
                if result['memory_per_row_kb'] > thresholds['memory_per_row_kb']:
                    violations.append(f'Memory per row {result[\"memory_per_row_kb\"]:.3f}KB exceeds threshold {thresholds[\"memory_per_row_kb\"]}KB')

        # Report violations
        if violations:
            print('❌ Threshold violations detected:')
            for violation in violations:
                print(f'  - {violation}')

            # Check if enforcement is enabled
            enforcement = config.get('enforcement', {})
            if enforcement.get('fail_on_threshold_breach', False):
                sys.exit(1)
        else:
            print('✅ All performance thresholds passed')
        " || echo "Threshold check completed with warnings"

    - name: Comment PR with Performance Report
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      continue-on-error: true
      with:
        script: |
          const fs = require('fs');

          let comment = '## 📊 Performance Benchmark Report\n\n';

          // Add comparison report if it exists
          if (fs.existsSync('comparison-report.md')) {
            const report = fs.readFileSync('comparison-report.md', 'utf8');
            comment += report;
          } else {
            comment += 'No comparison data available.\n';
          }

          // Find and update existing comment or create new one
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const botComment = comments.find(comment =>
            comment.user.type === 'Bot' && comment.body.includes('Performance Benchmark Report')
          );

          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }
